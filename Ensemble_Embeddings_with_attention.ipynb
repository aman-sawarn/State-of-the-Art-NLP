{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j2GTcNso0nq4"
   },
   "source": [
    "https://mlwhiz.com/blog/2019/02/19/siver_medal_kaggle_learnings/\n",
    "\n",
    "https://towardsdatascience.com/https-medium-com-tanaygahlot-moving-beyond-the-distributional-model-for-word-representation-b0823f1769f8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sQDLFg4Kf1w0"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lc = LancasterStemmer()\n",
    "from nltk.stem import SnowballStemmer\n",
    "sb = SnowballStemmer(\"english\")\n",
    "import gc\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate, LSTM, GRU\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "import sys\n",
    "from os.path import dirname\n",
    "#sys.path.append(dirname(dirname(__file__)))\n",
    "from keras import initializers\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras import backend as K\n",
    "\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "taynK1yPntgK"
   },
   "outputs": [],
   "source": [
    "# https://github.com/bfelbo/DeepMoji/blob/master/deepmoji/attlayer.py\n",
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # computes a probability distribution over the timesteps\n",
    "        # uses 'max trick' for numerical stability\n",
    "        # reshape is done to avoid issue with Tensorflow\n",
    "        # and 1-dimensional weights\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "\n",
    "        # masked timesteps have zero weight\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(att_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, att_weights]\n",
    "        return result\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "KlcGxRKWn02W",
    "outputId": "5e33d40e-027c-4290-9e08-79e217e84ca8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/cpmpml/spell-checker-using-word2vec\n",
    "spell_model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/My Drive/Colab Notebooks/wiki-news-300d-1M.vec')\n",
    "words = spell_model.index2word\n",
    "w_rank = {}\n",
    "for i,word in enumerate(words):\n",
    "    w_rank[word] = i\n",
    "WORDS = w_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RwtCcEs4n05A"
   },
   "outputs": [],
   "source": [
    "def words(text):\n",
    "   return re.findall(r'\\w+', text.lower())\n",
    "def P(word): \n",
    "    \"Probability of `word`.\"\n",
    "    # use inverse of rank as proxy\n",
    "    # returns 0 if the word isn't in the dictionary\n",
    "    return - WORDS.get(word, 0)\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or [word])\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "def singlify(word):\n",
    "    return \"\".join([letter for i,letter in enumerate(word) if i == 0 or letter != word[i-1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ANSM-f8oNQo"
   },
   "outputs": [],
   "source": [
    "# modified version of \n",
    "# https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings\n",
    "# https://www.kaggle.com/danofer/different-embeddings-with-attention-fork\n",
    "# https://www.kaggle.com/shujian/different-embeddings-with-attention-fork-fork\n",
    "def load_glove(word_dict, lemma_dict):\n",
    "    EMBEDDING_FILE = '/content/drive/My Drive/Colab Notebooks/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "    embed_size = 300\n",
    "    nb_words = len(word_dict)+1\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32)\n",
    "    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1.\n",
    "    print(unknown_vector[:5])\n",
    "    for key in tqdm(word_dict):\n",
    "        word = key\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.lower()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.upper()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.capitalize()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = ps.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lc.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = sb.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lemma_dict[key]\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        if len(key) > 1:\n",
    "            word = correction(key)\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[word_dict[key]] = embedding_vector\n",
    "                continue\n",
    "        embedding_matrix[word_dict[key]] = unknown_vector                    \n",
    "    return embedding_matrix, nb_words \n",
    "\n",
    "\n",
    "def load_fasttext(word_dict, lemma_dict):\n",
    "    EMBEDDING_FILE = '/content/drive/My Drive/Colab Notebooks/paragram_300_sl999.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
    "    embed_size = 300\n",
    "    nb_words = len(word_dict)+1\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32)\n",
    "    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1.\n",
    "    print(unknown_vector[:5])\n",
    "    for key in tqdm(word_dict):\n",
    "        word = key\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.lower()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.upper()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.capitalize()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = ps.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lc.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = sb.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lemma_dict[key]\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        if len(key) > 1:\n",
    "            word = correction(key)\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[word_dict[key]] = embedding_vector\n",
    "                continue\n",
    "        embedding_matrix[word_dict[key]] = unknown_vector                    \n",
    "    return embedding_matrix, nb_words \n",
    "\n",
    "\n",
    "def load_para(word_dict, lemma_dict):\n",
    "    EMBEDDING_FILE = '/content/drive/My Drive/Colab Notebooks/paragram_300_sl999.txt'\n",
    "    def get_coefs(word,*arr): \n",
    "      return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
    "    embed_size = 300\n",
    "    nb_words = len(word_dict)+1\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32)\n",
    "    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1.\n",
    "    print(unknown_vector[:5])\n",
    "    for key in tqdm(word_dict):\n",
    "        word = key\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.lower()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.upper()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.capitalize()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = ps.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lc.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = sb.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lemma_dict[key]\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        if len(key) > 1:\n",
    "            word = correction(key)\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[word_dict[key]] = embedding_vector\n",
    "                continue\n",
    "        embedding_matrix[word_dict[key]] = unknown_vector                    \n",
    "    return embedding_matrix, nb_words \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PsmjUtqJxlsY"
   },
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T-MbQyz2o2sE"
   },
   "outputs": [],
   "source": [
    "def build_model(embedding_matrix, nb_words, embedding_size=300):\n",
    "    inp = Input(shape=(max_length,))\n",
    "    x = Embedding(nb_words, embedding_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x1 = Bidirectional(CuDNNLSTM(256, return_sequences=True))(x)\n",
    "    x2 = Bidirectional(CuDNNGRU(128, return_sequences=True))(x1)\n",
    "    max_pool1 = GlobalMaxPooling1D()(x1)\n",
    "    max_pool2 = GlobalMaxPooling1D()(x2)\n",
    "    conc = Concatenate()([max_pool1, max_pool2])\n",
    "    predictions = Dense(1, activation='sigmoid')(conc)\n",
    "    model = Model(inputs=inp, outputs=predictions)\n",
    "    adam = optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O4WK7diFxilF"
   },
   "outputs": [],
   "source": [
    "def model_lstm_atten(embedding_matrix, nb_words, embedding_size=300):\n",
    "    inp = Input(shape=(max_length,))\n",
    "    x = Embedding(nb_words, embedding_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n",
    "    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "    x = AttentionWithContext()(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ZwTCmKXKo9av",
    "outputId": "e8b7bb9f-ba02-46d5-f79e-fa28fe34dd4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "--- 0.06915020942687988 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(\"Loading data ...\")\n",
    "train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/train.csv').fillna(' ')\n",
    "test = pd.read_csv('/content/drive/My Drive/Colab Notebooks/test.csv').fillna(' ')\n",
    "train_text = train['text']\n",
    "test_text = test['text']\n",
    "text_list = pd.concat([train_text, test_text])\n",
    "y = train['target'].values\n",
    "num_train_data = y.shape[0]\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "O52mMN_u2eE_",
    "outputId": "ec5972f7-c05e-4c18-b38a-452c9371d9bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean :  101.03743596479706\n",
      "median :  107.0\n",
      "mean :  102.10818265399939\n",
      "median :  109.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train['word_len']=train['text'].apply(lambda x:len(x))\n",
    "test['word_len']=test['text'].apply(lambda x:len(x))\n",
    "print(\"mean : \",train['word_len'].mean())\n",
    "print(\"median : \",train['word_len'].median())\n",
    "print(\"mean : \",test['word_len'].mean())\n",
    "print(\"median : \",test['word_len'].median())\n",
    "train['word_len']=train['text'].apply(lambda x:len(x))\n",
    "# print(train['word_len'].quantile(0.10))\n",
    "# print(train['word_len'].quantile(0.20))\n",
    "# print(train['word_len'].quantile(0.30))\n",
    "# print(train['word_len'].quantile(0.40))\n",
    "# print(train['word_len'].quantile(0.50))\n",
    "# print(train['word_len'].quantile(0.60))\n",
    "# print(train['word_len'].quantile(0.70))\n",
    "# print(train['word_len'].quantile(0.80))\n",
    "# print(train['word_len'].quantile(0.90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8_tH1Ongphrh"
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "c_821VkOo9dj",
    "outputId": "fa2393c6-6c18-4cff-b52a-2fc3da4c69df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy NLP ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10876it [00:04, 2268.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 41.63713026046753 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(\"Spacy NLP ...\")\n",
    "nlp = spacy.load('/content/drive/My Drive/Colab Notebooks/en_core_web_lg-2.1.0/en_core_web_lg/en_core_web_lg-2.1.0', disable=['parser','ner','tagger'])\n",
    "nlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)\n",
    "word_dict = {}\n",
    "word_index = 1\n",
    "lemma_dict = {}\n",
    "docs = nlp.pipe(text_list, n_threads = 2)\n",
    "word_sequences = []\n",
    "for doc in tqdm(docs):\n",
    "    word_seq = []\n",
    "    for token in doc:\n",
    "        if (token.text not in word_dict) and (token.pos_ is not \"PUNCT\"):\n",
    "            word_dict[token.text] = word_index\n",
    "            word_index += 1\n",
    "            lemma_dict[token.text] = token.lemma_\n",
    "        if token.pos_ is not \"PUNCT\":\n",
    "            word_seq.append(word_dict[token.text])\n",
    "    word_sequences.append(word_seq)\n",
    "del docs\n",
    "gc.collect()\n",
    "train_word_sequences = word_sequences[:num_train_data]\n",
    "test_word_sequences = word_sequences[num_train_data:]\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "7hT5Cjnjo9ga",
    "outputId": "4e8e4049-d00d-49ab-c0ca-c98f32e43492"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1   2   3 ...   0   0   0]\n",
      " [ 15  16  17 ...   0   0   0]\n",
      " [ 23  24  25 ...   0   0   0]\n",
      " ...\n",
      " [ 86  87  88 ...   0   0   0]\n",
      " [ 96  97  98 ...   0   0   0]\n",
      " [ 86  87 105 ...   0   0   0]]\n",
      "[[  46  762   57 ...    0    0    0]\n",
      " [8961  461    8 ...    0    0    0]\n",
      " [ 630  108   57 ...    0    0    0]\n",
      " ...\n",
      " [1697  168  323 ...    0    0    0]\n",
      " [ 157   57 2500 ...    0    0    0]\n",
      " [7827  972  168 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "max_length = 140\n",
    "embedding_size = 600\n",
    "learning_rate = 0.001\n",
    "batch_size = 512\n",
    "num_epoch = 4\n",
    "\n",
    "train_word_sequences = pad_sequences(train_word_sequences, maxlen=max_length, padding='post')\n",
    "test_word_sequences = pad_sequences(test_word_sequences, maxlen=max_length, padding='post')\n",
    "print(train_word_sequences[:10])\n",
    "print(test_word_sequences[:10])\n",
    "pred_prob = np.zeros((len(test_word_sequences),), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "9iqEvXT1o9jD",
    "outputId": "f2c66062-d996-4f26-b908-8d00065386e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding matrix ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1081/34498 [00:00<00:03, 10754.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. -1. -1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34498/34498 [00:05<00:00, 5828.81it/s]\n",
      "  4%|▎         | 1253/34498 [00:00<00:02, 12505.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. -1. -1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34498/34498 [00:06<00:00, 5416.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 205.25434064865112 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(\"Loading embedding matrix ...\")\n",
    "embedding_matrix_glove, nb_words = load_glove(word_dict, lemma_dict)\n",
    "embedding_matrix_fasttext, nb_words = load_fasttext(word_dict, lemma_dict)\n",
    "embedding_matrix = np.concatenate((embedding_matrix_glove, embedding_matrix_fasttext), axis=1)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 768
    },
    "colab_type": "code",
    "id": "1gLsED2lpx9q",
    "outputId": "cc81a2f5-dba2-4c1e-db6f-90d82ce7a828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training ...\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch 1/3\n",
      " - 10s - loss: 0.6170 - acc: 0.6585\n",
      "Epoch 2/3\n",
      " - 3s - loss: 0.4536 - acc: 0.7955\n",
      "Epoch 3/3\n",
      " - 3s - loss: 0.3950 - acc: 0.8273\n",
      "Epoch 1/1\n",
      " - 3s - loss: 0.3610 - acc: 0.8482\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:111: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "--- 28.643392086029053 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(\"Start training ...\")\n",
    "model = model_lstm_atten(embedding_matrix, nb_words, embedding_size)\n",
    "model.fit(train_word_sequences, y, batch_size=batch_size, epochs=num_epoch-1, verbose=2)\n",
    "pred_prob += 0.15*np.squeeze(model.predict(test_word_sequences, batch_size=batch_size, verbose=2))\n",
    "model.fit(train_word_sequences, y, batch_size=batch_size, epochs=1, verbose=2)\n",
    "pred_prob += 0.35*np.squeeze(model.predict(test_word_sequences, batch_size=batch_size, verbose=2))\n",
    "del model, embedding_matrix_fasttext, embedding_matrix\n",
    "gc.collect()\n",
    "K.clear_session()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "V9KIaKj-pyBs",
    "outputId": "fa52e84e-1d42-482b-95cc-935eb6b6eada"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding matrix ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1304/34498 [00:00<00:02, 13007.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. -1. -1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34498/34498 [00:05<00:00, 6326.23it/s]\n",
      "  4%|▍         | 1357/34498 [00:00<00:02, 13547.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. -1. -1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34498/34498 [00:05<00:00, 6385.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 20.552839279174805 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(\"Loading embedding matrix ...\")\n",
    "embedding_matrix_fasttext, nb_words = load_fasttext(word_dict, lemma_dict)\n",
    "embedding_matrix_para, nb_words = load_para(word_dict, lemma_dict)\n",
    "# embedding_matrix = np.concatenate((embedding_matrix_glove, embedding_matrix_para), axis=1)\n",
    "embedding_matrix = np.concatenate((0.60*embedding_matrix_glove, 0.40*embedding_matrix_fasttext), axis=1)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xKhKhGNWH7BR",
    "outputId": "3a9c7a74-bcf3-4132-b4f3-4c157295988c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34499, 600)"
      ]
     },
     "execution_count": 77,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bMmkyIUZp4N9"
   },
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "# print(\"Start training ...\")\n",
    "model = model_lstm_atten(embedding_matrix, nb_words, embedding_size)\n",
    "# model.fit(train_word_sequences, y, batch_size=batch_size, epochs=num_epoch-1, verbose=2)\n",
    "# pred_prob += 0.15*np.squeeze(model.predict(test_word_sequences, batch_size=batch_size, verbose=2))\n",
    "# model.fit(train_word_sequences, y, batch_size=batch_size, epochs=1, verbose=2)\n",
    "# pred_prob += 0.35*np.squeeze(model.predict(test_word_sequences, batch_size=batch_size, verbose=2))\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "UB6MauM242bx",
    "outputId": "21c90db1-2d69-48a6-f679-7e5be9990b5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5709 samples, validate on 1904 samples\n",
      "Epoch 1/15\n",
      " - 10s - loss: 0.6123 - acc: 0.6674 - val_loss: 0.4892 - val_acc: 0.7815\n",
      "Epoch 2/15\n",
      " - 3s - loss: 0.4654 - acc: 0.7942 - val_loss: 0.4669 - val_acc: 0.7847\n",
      "Epoch 3/15\n",
      " - 3s - loss: 0.4163 - acc: 0.8168 - val_loss: 0.4850 - val_acc: 0.7883\n",
      "Epoch 4/15\n",
      " - 3s - loss: 0.4007 - acc: 0.8255 - val_loss: 0.4540 - val_acc: 0.7962\n",
      "Epoch 5/15\n",
      " - 3s - loss: 0.3878 - acc: 0.8355 - val_loss: 0.4550 - val_acc: 0.7931\n",
      "Epoch 6/15\n",
      " - 3s - loss: 0.3740 - acc: 0.8397 - val_loss: 0.4492 - val_acc: 0.7952\n",
      "Epoch 7/15\n",
      " - 3s - loss: 0.3498 - acc: 0.8527 - val_loss: 0.4824 - val_acc: 0.7925\n",
      "Epoch 8/15\n",
      " - 3s - loss: 0.3324 - acc: 0.8628 - val_loss: 0.4913 - val_acc: 0.7862\n",
      "Epoch 9/15\n",
      " - 3s - loss: 0.3180 - acc: 0.8723 - val_loss: 0.4863 - val_acc: 0.7836\n",
      "Epoch 10/15\n",
      " - 3s - loss: 0.3018 - acc: 0.8774 - val_loss: 0.4934 - val_acc: 0.7873\n",
      "Epoch 11/15\n",
      " - 3s - loss: 0.2954 - acc: 0.8821 - val_loss: 0.5529 - val_acc: 0.7710\n",
      "Epoch 12/15\n",
      " - 3s - loss: 0.2937 - acc: 0.8847 - val_loss: 0.5652 - val_acc: 0.7815\n",
      "Epoch 13/15\n",
      " - 3s - loss: 0.2852 - acc: 0.8865 - val_loss: 0.5608 - val_acc: 0.7820\n",
      "Epoch 14/15\n",
      " - 3s - loss: 0.2462 - acc: 0.9042 - val_loss: 0.5800 - val_acc: 0.7815\n",
      "Epoch 15/15\n",
      " - 3s - loss: 0.2207 - acc: 0.9175 - val_loss: 0.6147 - val_acc: 0.7768\n",
      "<keras.callbacks.History object at 0x7ef93f22ff60>\n",
      "1 Fold Completed:  --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- \n",
      "Train on 5710 samples, validate on 1903 samples\n",
      "Epoch 1/15\n",
      " - 11s - loss: 0.6385 - acc: 0.6359 - val_loss: 0.5324 - val_acc: 0.7451\n",
      "Epoch 2/15\n",
      " - 3s - loss: 0.4850 - acc: 0.7823 - val_loss: 0.4724 - val_acc: 0.7872\n",
      "Epoch 3/15\n",
      " - 3s - loss: 0.4306 - acc: 0.8105 - val_loss: 0.4792 - val_acc: 0.7761\n",
      "Epoch 4/15\n",
      " - 3s - loss: 0.4073 - acc: 0.8219 - val_loss: 0.4670 - val_acc: 0.7924\n",
      "Epoch 5/15\n",
      " - 3s - loss: 0.3816 - acc: 0.8384 - val_loss: 0.4528 - val_acc: 0.8035\n",
      "Epoch 6/15\n",
      " - 3s - loss: 0.3703 - acc: 0.8438 - val_loss: 0.4715 - val_acc: 0.7951\n",
      "Epoch 7/15\n",
      " - 3s - loss: 0.3506 - acc: 0.8541 - val_loss: 0.4946 - val_acc: 0.7819\n",
      "Epoch 8/15\n",
      " - 3s - loss: 0.3448 - acc: 0.8546 - val_loss: 0.4775 - val_acc: 0.7972\n",
      "Epoch 9/15\n",
      " - 3s - loss: 0.3171 - acc: 0.8680 - val_loss: 0.5014 - val_acc: 0.7972\n",
      "Epoch 10/15\n",
      " - 3s - loss: 0.2996 - acc: 0.8760 - val_loss: 0.5047 - val_acc: 0.7846\n",
      "Epoch 11/15\n",
      " - 3s - loss: 0.2813 - acc: 0.8874 - val_loss: 0.5457 - val_acc: 0.7704\n",
      "Epoch 12/15\n",
      " - 3s - loss: 0.2674 - acc: 0.8911 - val_loss: 0.6259 - val_acc: 0.7535\n",
      "Epoch 13/15\n",
      " - 3s - loss: 0.2554 - acc: 0.8947 - val_loss: 0.5739 - val_acc: 0.7809\n",
      "Epoch 14/15\n",
      " - 3s - loss: 0.2254 - acc: 0.9116 - val_loss: 0.6276 - val_acc: 0.7625\n",
      "Epoch 15/15\n",
      " - 3s - loss: 0.1993 - acc: 0.9236 - val_loss: 0.6755 - val_acc: 0.7572\n",
      "<keras.callbacks.History object at 0x7ef93ec18ef0>\n",
      "2 Fold Completed:  --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- \n",
      "Train on 5710 samples, validate on 1903 samples\n",
      "Epoch 1/15\n",
      " - 11s - loss: 0.6401 - acc: 0.6371 - val_loss: 0.5863 - val_acc: 0.6926\n",
      "Epoch 2/15\n",
      " - 3s - loss: 0.4945 - acc: 0.7673 - val_loss: 0.5042 - val_acc: 0.7746\n",
      "Epoch 3/15\n",
      " - 3s - loss: 0.4326 - acc: 0.8093 - val_loss: 0.4842 - val_acc: 0.7877\n",
      "Epoch 4/15\n",
      " - 3s - loss: 0.4011 - acc: 0.8210 - val_loss: 0.4895 - val_acc: 0.7830\n",
      "Epoch 5/15\n",
      " - 3s - loss: 0.3737 - acc: 0.8385 - val_loss: 0.4913 - val_acc: 0.7819\n",
      "Epoch 6/15\n",
      " - 3s - loss: 0.3551 - acc: 0.8490 - val_loss: 0.5153 - val_acc: 0.7767\n",
      "Epoch 7/15\n",
      " - 3s - loss: 0.3427 - acc: 0.8525 - val_loss: 0.5246 - val_acc: 0.7646\n",
      "Epoch 8/15\n",
      " - 3s - loss: 0.3257 - acc: 0.8627 - val_loss: 0.5504 - val_acc: 0.7630\n",
      "Epoch 9/15\n",
      " - 3s - loss: 0.3134 - acc: 0.8665 - val_loss: 0.5688 - val_acc: 0.7593\n",
      "Epoch 10/15\n",
      " - 3s - loss: 0.2990 - acc: 0.8764 - val_loss: 0.5884 - val_acc: 0.7462\n",
      "Epoch 11/15\n",
      " - 3s - loss: 0.2844 - acc: 0.8860 - val_loss: 0.5946 - val_acc: 0.7488\n",
      "Epoch 12/15\n",
      " - 3s - loss: 0.2699 - acc: 0.8919 - val_loss: 0.6420 - val_acc: 0.7625\n",
      "Epoch 13/15\n",
      " - 3s - loss: 0.2434 - acc: 0.9082 - val_loss: 0.6793 - val_acc: 0.7320\n",
      "Epoch 14/15\n",
      " - 3s - loss: 0.2231 - acc: 0.9137 - val_loss: 0.6984 - val_acc: 0.7404\n",
      "Epoch 15/15\n",
      " - 3s - loss: 0.1939 - acc: 0.9291 - val_loss: 0.7932 - val_acc: 0.7399\n",
      "<keras.callbacks.History object at 0x7ef93dc7af98>\n",
      "3 Fold Completed:  --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- \n",
      "Train on 5710 samples, validate on 1903 samples\n",
      "Epoch 1/15\n",
      " - 11s - loss: 0.6340 - acc: 0.6333 - val_loss: 0.5049 - val_acc: 0.7667\n",
      "Epoch 2/15\n",
      " - 3s - loss: 0.4808 - acc: 0.7804 - val_loss: 0.4848 - val_acc: 0.7903\n",
      "Epoch 3/15\n",
      " - 3s - loss: 0.4504 - acc: 0.7960 - val_loss: 0.4145 - val_acc: 0.8056\n",
      "Epoch 4/15\n",
      " - 3s - loss: 0.4087 - acc: 0.8175 - val_loss: 0.4085 - val_acc: 0.8092\n",
      "Epoch 5/15\n",
      " - 3s - loss: 0.3912 - acc: 0.8342 - val_loss: 0.4149 - val_acc: 0.8114\n",
      "Epoch 6/15\n",
      " - 3s - loss: 0.3773 - acc: 0.8417 - val_loss: 0.4108 - val_acc: 0.8066\n",
      "Epoch 7/15\n",
      " - 3s - loss: 0.3598 - acc: 0.8503 - val_loss: 0.4383 - val_acc: 0.7940\n",
      "Epoch 8/15\n",
      " - 3s - loss: 0.3474 - acc: 0.8606 - val_loss: 0.4182 - val_acc: 0.8087\n",
      "Epoch 9/15\n",
      " - 3s - loss: 0.3451 - acc: 0.8578 - val_loss: 0.4467 - val_acc: 0.8008\n",
      "Epoch 10/15\n",
      " - 3s - loss: 0.3224 - acc: 0.8713 - val_loss: 0.4318 - val_acc: 0.8056\n",
      "Epoch 11/15\n",
      " - 3s - loss: 0.2988 - acc: 0.8795 - val_loss: 0.4736 - val_acc: 0.8035\n",
      "Epoch 12/15\n",
      " - 3s - loss: 0.2753 - acc: 0.8916 - val_loss: 0.5033 - val_acc: 0.8014\n",
      "Epoch 13/15\n",
      " - 3s - loss: 0.2487 - acc: 0.9058 - val_loss: 0.5079 - val_acc: 0.7972\n",
      "Epoch 14/15\n",
      " - 3s - loss: 0.2263 - acc: 0.9187 - val_loss: 0.5655 - val_acc: 0.7909\n",
      "Epoch 15/15\n",
      " - 3s - loss: 0.2102 - acc: 0.9219 - val_loss: 0.5834 - val_acc: 0.7898\n",
      "<keras.callbacks.History object at 0x7ef93d31c400>\n",
      "4 Fold Completed:  --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- \n",
      "55.42% (+/- 0.00%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "# from sklearn import model_selection   \n",
    "# kfold=model_selection.KFold\n",
    "skf = StratifiedKFold(n_splits=4)\n",
    "skf.get_n_splits(train_word_sequences, y)\n",
    "cvscores = []\n",
    "i=1\n",
    "for train_1, test_1 in skf.split(train_word_sequences, y):\n",
    "  model = model_lstm_atten(embedding_matrix, nb_words, embedding_size)\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# Fit the model\n",
    "  print(model.fit(train_word_sequences[train_1], y[train_1], epochs=15, batch_size=512, verbose=2, validation_data=(train_word_sequences[test_1], y[test_1])))\n",
    "\t# evaluate the model\n",
    "  # scores = model.evaluate(test_word_sequences[test], y[test], verbose=2)\n",
    "  # print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "  cvscores.append(scores[1] * 100)\n",
    "  print(i, \"Fold Completed: \",   \"--X-- \"*25)\n",
    "  i=i+1\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "drMFtDWDLcW6",
    "outputId": "95453318-2994-4d15-b586-7bf24d7cb15f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7456238361266293"
      ]
     },
     "execution_count": 95,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_prob=model.predict(test_word_sequences, batch_size=batch_size, verbose=2)\n",
    "f1_score(original['target'], submission['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pqey5liEp4Q2"
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "pred_prob=model.predict(test_word_sequences, batch_size=batch_size, verbose=2)\n",
    "submission['target'] = np.array(pred_prob.round())\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "original=pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/NLP Original.csv\")\n",
    "original['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "9vJrlV_Cp4UH",
    "outputId": "ff0e2a1c-6a70-454f-f262-0e5f1ac04ced"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    1980\n",
       "1.0    1283\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9WeS302hKRru",
    "outputId": "fd6fbb67-6387-4764-f554-524beca93e89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5709 samples, validate on 1904 samples\n",
      "Epoch 1/15\n",
      " - 28s - loss: 0.6389 - acc: 0.6257 - val_loss: 0.5758 - val_acc: 0.7489\n",
      "Epoch 2/15\n",
      " - 17s - loss: 0.4787 - acc: 0.7846 - val_loss: 0.4487 - val_acc: 0.7894\n",
      "Epoch 3/15\n",
      " - 17s - loss: 0.4242 - acc: 0.8122 - val_loss: 0.4462 - val_acc: 0.7978\n",
      "Epoch 4/15\n",
      " - 17s - loss: 0.3877 - acc: 0.8317 - val_loss: 0.4531 - val_acc: 0.7967\n",
      "Epoch 5/15\n",
      " - 17s - loss: 0.3717 - acc: 0.8443 - val_loss: 0.4525 - val_acc: 0.7952\n",
      "Epoch 6/15\n",
      " - 17s - loss: 0.3568 - acc: 0.8506 - val_loss: 0.4718 - val_acc: 0.7920\n",
      "Epoch 7/15\n",
      " - 17s - loss: 0.3459 - acc: 0.8544 - val_loss: 0.4686 - val_acc: 0.7899\n",
      "Epoch 8/15\n",
      " - 17s - loss: 0.3401 - acc: 0.8543 - val_loss: 0.4778 - val_acc: 0.7784\n",
      "Epoch 9/15\n",
      " - 17s - loss: 0.3413 - acc: 0.8574 - val_loss: 0.4742 - val_acc: 0.7904\n",
      "Epoch 10/15\n",
      " - 17s - loss: 0.3178 - acc: 0.8706 - val_loss: 0.5434 - val_acc: 0.7847\n",
      "Epoch 11/15\n",
      " - 17s - loss: 0.2942 - acc: 0.8797 - val_loss: 0.5057 - val_acc: 0.7831\n",
      "Epoch 12/15\n",
      " - 17s - loss: 0.2797 - acc: 0.8888 - val_loss: 0.5405 - val_acc: 0.7747\n",
      "Epoch 13/15\n",
      " - 17s - loss: 0.2559 - acc: 0.9030 - val_loss: 0.5790 - val_acc: 0.7694\n",
      "Epoch 14/15\n",
      " - 17s - loss: 0.2477 - acc: 0.9017 - val_loss: 0.5878 - val_acc: 0.7747\n",
      "Epoch 15/15\n",
      " - 17s - loss: 0.2155 - acc: 0.9175 - val_loss: 0.6317 - val_acc: 0.7721\n",
      "<keras.callbacks.History object at 0x7ef93c984940>\n",
      "1 Fold Completed:  --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- \n",
      "Train on 5710 samples, validate on 1903 samples\n",
      "Epoch 1/15\n",
      " - 28s - loss: 0.6387 - acc: 0.6263 - val_loss: 0.5356 - val_acc: 0.7320\n",
      "Epoch 2/15\n",
      " - 17s - loss: 0.4775 - acc: 0.7823 - val_loss: 0.4661 - val_acc: 0.8019\n",
      "Epoch 3/15\n",
      " - 17s - loss: 0.4201 - acc: 0.8158 - val_loss: 0.4860 - val_acc: 0.7930\n",
      "Epoch 4/15\n",
      " - 17s - loss: 0.4079 - acc: 0.8224 - val_loss: 0.4702 - val_acc: 0.7972\n",
      "Epoch 5/15\n",
      " - 17s - loss: 0.3790 - acc: 0.8345 - val_loss: 0.4622 - val_acc: 0.7961\n",
      "Epoch 6/15\n",
      " - 17s - loss: 0.3693 - acc: 0.8457 - val_loss: 0.4829 - val_acc: 0.7919\n",
      "Epoch 7/15\n",
      " - 17s - loss: 0.3524 - acc: 0.8529 - val_loss: 0.4944 - val_acc: 0.7919\n",
      "Epoch 8/15\n",
      " - 17s - loss: 0.3384 - acc: 0.8555 - val_loss: 0.4904 - val_acc: 0.7903\n",
      "Epoch 9/15\n",
      " - 17s - loss: 0.3341 - acc: 0.8585 - val_loss: 0.5850 - val_acc: 0.7672\n",
      "Epoch 10/15\n",
      " - 17s - loss: 0.3231 - acc: 0.8650 - val_loss: 0.5166 - val_acc: 0.7803\n",
      "Epoch 11/15\n",
      " - 17s - loss: 0.2962 - acc: 0.8795 - val_loss: 0.6022 - val_acc: 0.7625\n",
      "Epoch 12/15\n",
      " - 17s - loss: 0.2918 - acc: 0.8806 - val_loss: 0.5793 - val_acc: 0.7630\n",
      "Epoch 13/15\n",
      " - 17s - loss: 0.2552 - acc: 0.8989 - val_loss: 0.6151 - val_acc: 0.7588\n",
      "Epoch 14/15\n",
      " - 17s - loss: 0.2433 - acc: 0.9096 - val_loss: 0.7145 - val_acc: 0.7588\n",
      "Epoch 15/15\n",
      " - 17s - loss: 0.2900 - acc: 0.8765 - val_loss: 0.5575 - val_acc: 0.7667\n",
      "<keras.callbacks.History object at 0x7ef93bf555f8>\n",
      "2 Fold Completed:  --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- \n",
      "Train on 5710 samples, validate on 1903 samples\n",
      "Epoch 1/15\n",
      " - 30s - loss: 0.6197 - acc: 0.6595 - val_loss: 0.5305 - val_acc: 0.7556\n",
      "Epoch 2/15\n",
      " - 17s - loss: 0.4700 - acc: 0.7900 - val_loss: 0.5016 - val_acc: 0.7751\n",
      "Epoch 3/15\n",
      " - 17s - loss: 0.4179 - acc: 0.8145 - val_loss: 0.4834 - val_acc: 0.7751\n",
      "Epoch 4/15\n",
      " - 17s - loss: 0.3847 - acc: 0.8312 - val_loss: 0.4866 - val_acc: 0.7761\n",
      "Epoch 5/15\n",
      " - 17s - loss: 0.3697 - acc: 0.8378 - val_loss: 0.5010 - val_acc: 0.7620\n",
      "Epoch 6/15\n",
      " - 17s - loss: 0.3524 - acc: 0.8455 - val_loss: 0.5048 - val_acc: 0.7814\n",
      "Epoch 7/15\n",
      " - 17s - loss: 0.3319 - acc: 0.8564 - val_loss: 0.5508 - val_acc: 0.7588\n",
      "Epoch 8/15\n",
      " - 17s - loss: 0.3321 - acc: 0.8595 - val_loss: 0.5605 - val_acc: 0.7541\n",
      "Epoch 9/15\n",
      " - 17s - loss: 0.3218 - acc: 0.8620 - val_loss: 0.5457 - val_acc: 0.7614\n",
      "Epoch 10/15\n",
      " - 17s - loss: 0.2899 - acc: 0.8760 - val_loss: 0.5778 - val_acc: 0.7493\n",
      "Epoch 11/15\n",
      " - 17s - loss: 0.2751 - acc: 0.8867 - val_loss: 0.5780 - val_acc: 0.7551\n",
      "Epoch 12/15\n",
      " - 17s - loss: 0.2694 - acc: 0.8898 - val_loss: 0.6498 - val_acc: 0.7404\n",
      "Epoch 13/15\n",
      " - 17s - loss: 0.2426 - acc: 0.9016 - val_loss: 0.6576 - val_acc: 0.7504\n",
      "Epoch 14/15\n",
      " - 17s - loss: 0.2192 - acc: 0.9096 - val_loss: 0.7312 - val_acc: 0.7378\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-fdf2716337fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_word_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_word_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m# evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;31m# scores = model.evaluate(test_word_sequences[test], y[test], verbose=2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def model_lstm_atten(embedding_matrix, nb_words, embedding_size=300):\n",
    "    inp = Input(shape=(max_length,))\n",
    "    x = Embedding(nb_words, embedding_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "    x = AttentionWithContext()(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "skf = StratifiedKFold(n_splits=4)\n",
    "skf.get_n_splits(train_word_sequences, y)\n",
    "cvscores = []\n",
    "i=1\n",
    "for train_1, test_1 in skf.split(train_word_sequences, y):\n",
    "  model = model_lstm_atten(embedding_matrix, nb_words, embedding_size)\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# Fit the model\n",
    "  print(model.fit(train_word_sequences[train_1], y[train_1], epochs=15, batch_size=512, verbose=2, validation_data=(train_word_sequences[test_1], y[test_1])))\n",
    "\t# evaluate the model\n",
    "  # scores = model.evaluate(test_word_sequences[test], y[test], verbose=2)\n",
    "  # print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "  cvscores.append(scores[1] * 100)\n",
    "  print(i, \"Fold Completed: \",   \"--X-- \"*25)\n",
    "  i=i+1\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_gwbxSBoM1hZ",
    "outputId": "83f9b01f-2473-4f16-b2a9-bc8e92538cdc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7456238361266293"
      ]
     },
     "execution_count": 99,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_prob=model.predict(test_word_sequences, batch_size=batch_size, verbose=2)\n",
    "f1_score(original['target'], submission['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "QhKODlWKM7mo",
    "outputId": "c264229c-3cf2-43d3-fdcd-10930a1875d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5709 samples, validate on 1904 samples\n",
      "Epoch 1/15\n",
      " - 30s - loss: 0.6434 - acc: 0.6274 - val_loss: 0.5683 - val_acc: 0.7600\n",
      "Epoch 2/15\n",
      " - 17s - loss: 0.5003 - acc: 0.7761 - val_loss: 0.4748 - val_acc: 0.7857\n",
      "Epoch 3/15\n",
      " - 17s - loss: 0.4202 - acc: 0.8164 - val_loss: 0.4911 - val_acc: 0.7857\n",
      "Epoch 4/15\n",
      " - 17s - loss: 0.4011 - acc: 0.8206 - val_loss: 0.5142 - val_acc: 0.7789\n",
      "Epoch 5/15\n",
      " - 17s - loss: 0.3919 - acc: 0.8297 - val_loss: 0.4590 - val_acc: 0.7925\n",
      "Epoch 6/15\n",
      " - 17s - loss: 0.3683 - acc: 0.8431 - val_loss: 0.4494 - val_acc: 0.7952\n",
      "Epoch 7/15\n",
      " - 17s - loss: 0.3545 - acc: 0.8508 - val_loss: 0.4579 - val_acc: 0.7946\n",
      "Epoch 8/15\n",
      " - 17s - loss: 0.3393 - acc: 0.8592 - val_loss: 0.4627 - val_acc: 0.7920\n",
      "Epoch 9/15\n",
      " - 17s - loss: 0.3357 - acc: 0.8625 - val_loss: 0.4717 - val_acc: 0.7878\n",
      "Epoch 10/15\n",
      " - 17s - loss: 0.3243 - acc: 0.8721 - val_loss: 0.5172 - val_acc: 0.7810\n",
      "Epoch 11/15\n",
      " - 17s - loss: 0.3144 - acc: 0.8704 - val_loss: 0.5191 - val_acc: 0.7778\n",
      "Epoch 12/15\n",
      " - 17s - loss: 0.2865 - acc: 0.8875 - val_loss: 0.5477 - val_acc: 0.7820\n",
      "Epoch 13/15\n",
      " - 17s - loss: 0.2658 - acc: 0.8996 - val_loss: 0.5642 - val_acc: 0.7689\n",
      "Epoch 14/15\n",
      " - 17s - loss: 0.2614 - acc: 0.8993 - val_loss: 0.5839 - val_acc: 0.7616\n",
      "Epoch 15/15\n",
      " - 17s - loss: 0.2374 - acc: 0.9112 - val_loss: 0.6219 - val_acc: 0.7731\n",
      "<keras.callbacks.History object at 0x7ef8c839b0f0>\n",
      "1 Fold Completed:  --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- \n",
      "Train on 5710 samples, validate on 1903 samples\n",
      "Epoch 1/15\n",
      " - 31s - loss: 0.6553 - acc: 0.5984 - val_loss: 0.5746 - val_acc: 0.7483\n",
      "Epoch 2/15\n",
      " - 17s - loss: 0.4949 - acc: 0.7772 - val_loss: 0.4572 - val_acc: 0.8029\n",
      "Epoch 3/15\n",
      " - 17s - loss: 0.4331 - acc: 0.8074 - val_loss: 0.4553 - val_acc: 0.7951\n",
      "Epoch 4/15\n",
      " - 17s - loss: 0.4054 - acc: 0.8275 - val_loss: 0.4744 - val_acc: 0.7919\n",
      "Epoch 5/15\n",
      " - 17s - loss: 0.3822 - acc: 0.8352 - val_loss: 0.4519 - val_acc: 0.7945\n",
      "Epoch 6/15\n",
      " - 17s - loss: 0.3664 - acc: 0.8426 - val_loss: 0.4939 - val_acc: 0.7809\n",
      "Epoch 7/15\n",
      " - 17s - loss: 0.3568 - acc: 0.8469 - val_loss: 0.4865 - val_acc: 0.7893\n",
      "Epoch 8/15\n",
      " - 17s - loss: 0.3352 - acc: 0.8594 - val_loss: 0.4687 - val_acc: 0.8029\n",
      "Epoch 9/15\n",
      " - 17s - loss: 0.3174 - acc: 0.8653 - val_loss: 0.5044 - val_acc: 0.7867\n",
      "Epoch 10/15\n",
      " - 17s - loss: 0.3112 - acc: 0.8709 - val_loss: 0.5243 - val_acc: 0.7888\n",
      "Epoch 11/15\n",
      " - 17s - loss: 0.2834 - acc: 0.8844 - val_loss: 0.5815 - val_acc: 0.7714\n",
      "Epoch 12/15\n",
      " - 18s - loss: 0.2793 - acc: 0.8807 - val_loss: 0.5185 - val_acc: 0.7846\n",
      "Epoch 13/15\n",
      " - 18s - loss: 0.2533 - acc: 0.8982 - val_loss: 0.5811 - val_acc: 0.7772\n",
      "Epoch 14/15\n",
      " - 18s - loss: 0.2361 - acc: 0.9026 - val_loss: 0.6697 - val_acc: 0.7651\n",
      "Epoch 15/15\n",
      " - 18s - loss: 0.2230 - acc: 0.9103 - val_loss: 0.6236 - val_acc: 0.7772\n",
      "<keras.callbacks.History object at 0x7ef8c7f847b8>\n",
      "2 Fold Completed:  --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- \n",
      "Train on 5710 samples, validate on 1903 samples\n",
      "Epoch 1/15\n",
      " - 32s - loss: 0.6586 - acc: 0.5930 - val_loss: 0.6106 - val_acc: 0.7115\n",
      "Epoch 2/15\n",
      " - 17s - loss: 0.5193 - acc: 0.7720 - val_loss: 0.5077 - val_acc: 0.7651\n",
      "Epoch 3/15\n",
      " - 17s - loss: 0.4255 - acc: 0.8095 - val_loss: 0.4800 - val_acc: 0.7861\n",
      "Epoch 4/15\n",
      " - 17s - loss: 0.3912 - acc: 0.8266 - val_loss: 0.4951 - val_acc: 0.7756\n",
      "Epoch 5/15\n",
      " - 17s - loss: 0.3719 - acc: 0.8417 - val_loss: 0.4929 - val_acc: 0.7777\n",
      "Epoch 6/15\n",
      " - 17s - loss: 0.3562 - acc: 0.8485 - val_loss: 0.4997 - val_acc: 0.7740\n",
      "Epoch 7/15\n",
      " - 17s - loss: 0.3405 - acc: 0.8553 - val_loss: 0.5891 - val_acc: 0.7546\n",
      "Epoch 8/15\n",
      " - 17s - loss: 0.3291 - acc: 0.8592 - val_loss: 0.5327 - val_acc: 0.7688\n",
      "Epoch 9/15\n",
      " - 17s - loss: 0.3115 - acc: 0.8716 - val_loss: 0.5638 - val_acc: 0.7620\n",
      "Epoch 10/15\n",
      " - 17s - loss: 0.2877 - acc: 0.8797 - val_loss: 0.5987 - val_acc: 0.7551\n",
      "Epoch 11/15\n",
      " - 17s - loss: 0.2669 - acc: 0.8907 - val_loss: 0.5974 - val_acc: 0.7530\n",
      "Epoch 12/15\n",
      " - 17s - loss: 0.2692 - acc: 0.8914 - val_loss: 0.7008 - val_acc: 0.7047\n",
      "Epoch 13/15\n",
      " - 17s - loss: 0.2750 - acc: 0.8900 - val_loss: 0.6078 - val_acc: 0.7599\n",
      "Epoch 14/15\n",
      " - 17s - loss: 0.2281 - acc: 0.9119 - val_loss: 0.7113 - val_acc: 0.7283\n",
      "Epoch 15/15\n",
      " - 17s - loss: 0.2215 - acc: 0.9124 - val_loss: 0.7342 - val_acc: 0.7346\n",
      "<keras.callbacks.History object at 0x7ef8c71fbf60>\n",
      "3 Fold Completed:  --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- \n",
      "Train on 5710 samples, validate on 1903 samples\n",
      "Epoch 1/15\n",
      " - 33s - loss: 0.6614 - acc: 0.6046 - val_loss: 0.5982 - val_acc: 0.7535\n",
      "Epoch 2/15\n",
      " - 17s - loss: 0.5452 - acc: 0.7641 - val_loss: 0.4660 - val_acc: 0.7951\n",
      "Epoch 3/15\n",
      " - 18s - loss: 0.4500 - acc: 0.8016 - val_loss: 0.4399 - val_acc: 0.7961\n",
      "Epoch 4/15\n",
      " - 17s - loss: 0.4190 - acc: 0.8180 - val_loss: 0.4168 - val_acc: 0.8071\n",
      "Epoch 5/15\n",
      " - 17s - loss: 0.3984 - acc: 0.8317 - val_loss: 0.4055 - val_acc: 0.8129\n",
      "Epoch 6/15\n",
      " - 18s - loss: 0.3752 - acc: 0.8473 - val_loss: 0.4150 - val_acc: 0.7977\n",
      "Epoch 7/15\n",
      " - 17s - loss: 0.3635 - acc: 0.8531 - val_loss: 0.4263 - val_acc: 0.8077\n",
      "Epoch 8/15\n",
      " - 17s - loss: 0.3500 - acc: 0.8560 - val_loss: 0.4655 - val_acc: 0.7935\n",
      "Epoch 9/15\n",
      " - 18s - loss: 0.3516 - acc: 0.8497 - val_loss: 0.4284 - val_acc: 0.7930\n",
      "Epoch 10/15\n",
      " - 17s - loss: 0.3232 - acc: 0.8743 - val_loss: 0.4338 - val_acc: 0.8014\n",
      "Epoch 11/15\n",
      " - 17s - loss: 0.3032 - acc: 0.8849 - val_loss: 0.4726 - val_acc: 0.7945\n",
      "Epoch 12/15\n",
      " - 17s - loss: 0.2898 - acc: 0.8891 - val_loss: 0.4788 - val_acc: 0.7998\n",
      "Epoch 13/15\n",
      " - 17s - loss: 0.2842 - acc: 0.8898 - val_loss: 0.4644 - val_acc: 0.7993\n",
      "Epoch 14/15\n",
      " - 17s - loss: 0.2530 - acc: 0.9040 - val_loss: 0.5961 - val_acc: 0.7824\n",
      "Epoch 15/15\n",
      " - 17s - loss: 0.2354 - acc: 0.9107 - val_loss: 0.5576 - val_acc: 0.7956\n",
      "<keras.callbacks.History object at 0x7ef8c59f9cf8>\n",
      "4 Fold Completed:  --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- --X-- \n",
      "55.42% (+/- 0.00%)\n"
     ]
    }
   ],
   "source": [
    "def model_lstm_atten(embedding_matrix, nb_words, embedding_size=300):\n",
    "    inp = Input(shape=(max_length,))\n",
    "    x = Embedding(nb_words, embedding_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = Bidirectional(LSTM(100, return_sequences=True))(x)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "    x = AttentionWithContext()(x)\n",
    "    x = Dense(48, activation=\"relu\")(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "skf = StratifiedKFold(n_splits=4)\n",
    "skf.get_n_splits(train_word_sequences, y)\n",
    "cvscores = []\n",
    "i=1\n",
    "for train_1, test_1 in skf.split(train_word_sequences, y):\n",
    "  model = model_lstm_atten(embedding_matrix, nb_words, embedding_size)\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# Fit the model\n",
    "  print(model.fit(train_word_sequences[train_1], y[train_1], epochs=15, batch_size=512, verbose=2, validation_data=(train_word_sequences[test_1], y[test_1])))\n",
    "\t# evaluate the model\n",
    "  # scores = model.evaluate(test_word_sequences[test], y[test], verbose=2)\n",
    "  # print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "  cvscores.append(scores[1] * 100)\n",
    "  print(i, \"Fold Completed: \",   \"--X-- \"*25)\n",
    "  i=i+1\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "87ncjeymNNeF",
    "outputId": "58db9d09-84d5-4f47-eefa-2ec9cbe5948e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7456238361266293"
      ]
     },
     "execution_count": 101,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_prob=model.predict(test_word_sequences, batch_size=batch_size, verbose=2)\n",
    "f1_score(original['target'], submission['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8wKUDjhNNRmk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Ensemble Embeddings with attention.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
